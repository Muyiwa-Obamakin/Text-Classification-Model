# üé≠ Text-Classification-Model
![Python](https://img.shields.io/badge/Python-3.10-3776AB?style=for-the-badge&logo=python&logoColor=white) ![Scikit-Learn](https://img.shields.io/badge/scikit_learn-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white) ![Jupyter](https://img.shields.io/badge/Jupyter-F37626?style=for-the-badge&logo=jupyter&logoColor=white) ![Pandas](https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white)

### üìå Project Overview
This project implements a Natural Language Processing (NLP) pipeline designed to classify textual data (X Tweets) into six distinct emotional categories: **Sadness, Joy, Love, Anger, Fear, and Surprise**.

The goal was to build a robust model capable of understanding the sentiment behind short form text, which has applications in customer feedback analysis, social media monitoring, and mental health assessment.

---

## ‚öôÔ∏è Methodology & Design Choices

### 1. Data Preprocessing
Raw text data is noisy. To improve model performance, the following cleaning steps were applied *before* vectorization:
* **Regex Cleaning:** Removed non-alphabetical characters, URLs, and handles to focus purely on emotional content.
* **Lowercasing:** Standardized text to treat "Happy" and "happy" as identical features.
* **Stopword Removal:** Used NLTK's list to remove high-frequency, low-meaning words (e.g., "the", "is") to reduce noise.
* **Stemming (PorterStemmer):** Reduced words to their root form (e.g., "loving" ‚Üí "lov") to decrease feature dimensionality.

### 2. Feature Engineering (TF-IDF)
Instead of simple Bag-of-Words (count) vectorization, I utilized **TF-IDF (Term Frequency-Inverse Document Frequency)**.
* **Why?** TF-IDF assigns lower weights to words that appear frequently across all documents (generic words) and higher weights to unique, emotionally charged words. This significantly improved the model's ability to distinguish between subtle emotions like "Fear" vs. "Surprise."

### 3. Model Selection
Three algorithms were evaluated for this classification task:
1.  **Logistic Regression (Selected Model)**
2.  Linear Support Vector Classifier (Linear SVC)
3.  Multinomial Naive Bayes

**Why Logistic Regression?**
While Naive Bayes is a common baseline for NLP, **Logistic Regression outperformed it significantly (approx. 90% vs 77%)**. Logistic Regression handled the high-dimensional sparse data generated by TF-IDF effectively and provided the best balance of training speed and interpretable probability outputs.

---

## üìä Key Findings & Results

The final Logistic Regression model achieved a **Mean Accuracy of ~80%** using 5-Fold Cross-Validation.

### üîç Performance Analysis

* **Class Separation:**
    The **Confusion Matrix** shows strong diagonal performance, indicating the model correctly predicts the majority of classes.
    * *Strongest Class:* **Joy** (High Precision/Recall).
    * *Common Confusion:* There is slight overlap between **Love** and **Joy**, which is expected as they share similar vocabulary (positive sentiment).

* **Robustness (Cross-Validation):**
    The Cross-Validation scores (visualized in the repo) show a stable performance across 5 different folds (ranging between ~88% and ~90%). This "W" shape in the accuracy plot confirms the model is not overfitting to a specific subset of data and generalizes well.

* **Precision vs. Recall:**
    The Precision-Recall curves demonstrate that the model maintains high precision even as recall increases, particularly for the distinct negative emotions like **Anger** and **Sadness**.

---

## üèÅ Conclusion

This project successfully demonstrates that a linear model (Logistic Regression), when paired with effective text preprocessing and TF-IDF vectorization, can achieve great results on text classification tasks without the computational cost of deep neural networks.

**Future Improvements:**
* Addressing the minor confusion between "Love" and "Joy" by introducing n-gram features (capturing phrases like "falling in love" vs "jumping for joy").
* Experimenting with transformer-based models (BERT) to capture deeper contextual nuances.

---
